{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your model\n",
    "from torchvision.models import resnet18, resnet50, ResNet50_Weights, ResNet18_Weights\n",
    "from torchcam.methods import SmoothGradCAMpp\n",
    "from torchvision.transforms.functional import normalize, resize\n",
    "from torchvision.models import resnet18\n",
    "from moviepy.editor import *\n",
    "from crop import process_frame_custom\n",
    "import numpy as np\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:no value was provided for `target_layer`, thus set to 'layer4'.\n",
      "WARNING:py.warnings:/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/moviepy/video/io/ffmpeg_reader.py:123: UserWarning: Warning: in file /Users/adiprabs/sample.mp4, 2764800 bytes wanted but 0 bytes read,at frame 1801/1802, at time 60.03/60.05 sec. Using the last valid frame instead.\n",
      "  warnings.warn(\"Warning: in file %s, \"%(self.filename)+\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video output_video13.mp4.\n",
      "MoviePy - Writing audio in output_video13TEMP_MPY_wvf_snd.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video output_video13.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready output_video13.mp4\n"
     ]
    }
   ],
   "source": [
    "aspect_ratio = (9, 16)\n",
    "input_video = \"/Users/adiprabs/sample.mp4\"\n",
    "\n",
    "# Output video file\n",
    "output_video = \"output_video13.mp4\"\n",
    "\n",
    "# Desired aspect ratio\n",
    "    # Example aspect ratio\n",
    "\n",
    "# Function to crop and adjust center of the frame\n",
    "\n",
    "# Open input video\n",
    "video_clip = VideoFileClip(input_video)\n",
    "audio_clip = video_clip.audio\n",
    "frames = video_clip.iter_frames()\n",
    "\n",
    "\n",
    "processed_frames = []\n",
    "height, width = video_clip.get_frame(1).shape[:2]\n",
    "crop_height = height\n",
    "crop_width = int(height*aspect_ratio[0]/aspect_ratio[1])\n",
    "center = int(width/2)\n",
    "i = 0\n",
    "n_frames = video_clip.reader.nframes\n",
    "model = resnet18(weights=ResNet18_Weights.DEFAULT).eval()\n",
    "frequency = 3 # Adjusts after how many frames the center is readjusted based on the attention model\n",
    "counter = frequency\n",
    "transition_factor = 0.8 # The higher, the faster it moves between points of interest, must be positive\n",
    "if transition_factor <= 0:\n",
    "    raise Exception(\"MUST be positive\")\n",
    "\n",
    "with SmoothGradCAMpp(model) as cam_extractor:\n",
    "    for frame in frames:\n",
    "        if counter == frequency:\n",
    "            counter = 0\n",
    "            frame1 = torch.from_numpy(np.array(frame)).permute(2, 0, 1)\n",
    "\n",
    "            # Preprocess it for your chosen model\n",
    "            input_tensor = normalize(resize(frame1, (224, 224)) / 255., [0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "\n",
    "            # Preprocess your data and feed it to the model\n",
    "            out = model(input_tensor.unsqueeze(0))\n",
    "            # Retrieve the CAM by passing the class index and the model output\n",
    "            activation_map = cam_extractor(out.squeeze(0).argmax().item(), out)\n",
    "            \n",
    "            b = np.array(activation_map[0][0])\n",
    "            sumx = 0\n",
    "            sumy = 0\n",
    "            total_sum = np.sum(b) # Add a small constant to avoid division by zero\n",
    "            for x in range(len(b)):\n",
    "                for y in range(len(b[0])):\n",
    "                    sumx += b[x, y] * (x + 0)\n",
    "                    sumy += b[x, y] * (y + 0)\n",
    "            avgx = sumx / np.sum(b)\n",
    "            avgy = sumy / np.sum(b)\n",
    "            average_coords = (avgx, avgy)\n",
    "            # if (width * avgx / 7) > center:\n",
    "            #     center += 5\n",
    "            # if (width * avgx / 7) < center:\n",
    "            #     center -= 5\n",
    "            center = int(((1/transition_factor)*center + (width * avgx / 7))/(1 + 1/transition_factor))\n",
    "        processed_frames.append(process_frame_custom(frame, center))\n",
    "        counter += 1\n",
    "\n",
    "\n",
    "output_clip = ImageSequenceClip(processed_frames, fps=video_clip.fps)\n",
    "output_clip = output_clip.set_audio(audio_clip)\n",
    "output_clip.write_videofile(output_video,fps=video_clip.fps, codec=\"mpeg4\", audio_codec=\"aac\")\n",
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
